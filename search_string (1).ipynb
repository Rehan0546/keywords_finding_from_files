{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "search_string.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "it will read all text file from given folder,\n",
        "and create folder for each file and save, word cloud, word counting, keyword in files,\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "hB61KOMi262L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_location = '/content/drive/MyDrive/text_tech_classification'\n",
        "min_counts_for_keywords = 2 # use words that appear minimum 2 times in docoment.\n",
        "key_word_score = 0.99 # 90%"
      ],
      "metadata": {
        "id": "2jJWdv8tzQkX"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "import re\n",
        "import nltk\n",
        "# downlaoding nltk stop words\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('words')\n",
        "from wordcloud import WordCloud\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from nltk.probability import FreqDist\n",
        "import os, glob\n",
        "from nltk.corpus import words\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtL1DE_ZehL4",
        "outputId": "26336dd6-ba43-4647-a76b-876b88333410"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def paragraph_process(data):\n",
        "  data = data.split(';')\n",
        "  all_data = []\n",
        "  for i in data:\n",
        "    all_data.extend(i.split('\\t'))\n",
        "  data = []\n",
        "  for i in all_data:\n",
        "    data.extend(i.split('.'))\n",
        "  all_data = []\n",
        "  for i in data:\n",
        "    if not len(i.strip().split()) <= 1:\n",
        "      all_data.append(i.lower())\n",
        "  return all_data\n",
        "\n",
        "def file_process(data):\n",
        "  all_data = []\n",
        "  for i in data:\n",
        "    all_data.extend(paragraph_process(i))\n",
        "  return all_data \n",
        "\n",
        "# function to clean stop words\n",
        "def cleaning_stopwords(text, words_dict = set(words.words()),\n",
        "                       STOPWORDS = set(stopwords.words('english'))):\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS if word in  words_dict if len(word)>2])\n",
        "# Function to clean punctuation\n",
        "def cleaning_punctuations(text, punctuations_list = string.punctuation):\n",
        "    translator = str.maketrans('', '', punctuations_list)\n",
        "    return text.translate(translator)\n",
        "# Funvtion to clean repreating characters\n",
        "def cleaning_repeating_char(text):\n",
        "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
        "# Funtion to removing email and mentions signs\n",
        "def cleaning_email(data):\n",
        "    return re.sub('@[^\\s]+', ' ', data)\n",
        "# Cleaning web urls\n",
        "def cleaning_URLs(data):\n",
        "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))',' ',data)\n",
        "# Cleaning numebrs \n",
        "def cleaning_numbers(data):\n",
        "    return re.sub('[0-9]+', '', data)\n",
        "# Function to stemming all words\n",
        "def stemming_on_text(data, st = nltk.PorterStemmer()):\n",
        "    text = [st.stem(word) for word in data]\n",
        "    return text\n",
        "# Function to lemmatization all words\n",
        "def lemmatizer_on_text(data, lm = nltk.WordNetLemmatizer()):\n",
        "    text = [lm.lemmatize(word) for word in data]\n",
        "    return text\n",
        "# Funtion to apply all cleaning function at once\n",
        "def preprocesses_data(data, tokenizer = RegexpTokenizer(r'\\w+')):\n",
        "  x = cleaning_stopwords(data)\n",
        "  x = cleaning_punctuations(x)\n",
        "  x = cleaning_email(x)\n",
        "  x = cleaning_URLs(x)\n",
        "  x = cleaning_numbers(x)\n",
        "  x =  tokenizer.tokenize(x)\n",
        "  # We are not doing stemming here because we need to make word thier base word by understading context of the data.\n",
        "  # That's why we choose lammatization\n",
        "  \n",
        "  # data= data.apply(lambda x: stemming_on_text(x))\n",
        "  x = lemmatizer_on_text(x)\n",
        "  # data = data.apply(lambda x: check_words(x))\n",
        "  x = ' '.join(x)\n",
        "  # print(x)\n",
        "  return x"
      ],
      "metadata": {
        "id": "DI0sQdw_eYZ8"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file_name):\n",
        "    print(file_name)\n",
        "    with open(file_name, \"r\") as txt_file:\n",
        "        data = txt_file.readlines()\n",
        "    data = file_process(data)\n",
        "    tqdm.pandas(desc = 'Cleaning data')\n",
        "    data = pd.Series(data).progress_apply(lambda x: preprocesses_data(x))\n",
        "\n",
        "    data = list(filter(None, data))\n",
        "    data = \" \".join(data)\n",
        "\n",
        "    fdist = FreqDist(data.split())\n",
        "    most_common = fdist.most_common(20)\n",
        "    print(most_common)\n",
        "    most_common = list(reversed(most_common))\n",
        "\n",
        "    counts = np.array(most_common)[:,1]\n",
        "    words = np.array(most_common)[:,0]\n",
        "\n",
        "    plt.figure(figsize = (15,7))\n",
        "    plt.bar(list(words), list(counts))\n",
        "    plt.xticks(rotation='90')\n",
        "    plt.title('Most common words')\n",
        "    plt.show()\n",
        "    plt.savefig('Most common words.png')\n",
        "\n",
        "    # Generate a word cloud image\n",
        "    wordcloud = WordCloud().generate(data)\n",
        "    plt.figure(figsize = (15,7))\n",
        "    plt.title('words Word cloud')\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.show()\n",
        "    plt.savefig('words Word cloud.png')\n",
        "\n",
        "    word2vec = Word2Vec([data.split()], min_count=min_counts_for_keywords)\n",
        "\n",
        "    all_words = []\n",
        "    for word in tqdm(data.split(),desc='Finding keywords'):\n",
        "      try:\n",
        "        sim_words = word2vec.wv.most_similar(word)\n",
        "        for sim_word in sim_words:\n",
        "          if sim_word[1]>key_word_score:\n",
        "            all_words.append(word+' '+ sim_word[0])\n",
        "      except:\n",
        "        pass\n",
        "    fdist = FreqDist(all_words)\n",
        "    most_common = fdist.most_common(20)\n",
        "    print(most_common)\n",
        "    most_common = list(reversed(most_common))\n",
        "\n",
        "    counts = np.array(most_common)[:,1]\n",
        "    words = np.array(most_common)[:,0]\n",
        "\n",
        "    plt.figure(figsize = (15,7))\n",
        "    plt.bar(list(words), list(counts))\n",
        "    plt.xticks(rotation='90')\n",
        "    plt.title('Most common keywords')\n",
        "    plt.show()\n",
        "    plt.savefig('Most common keywords.png')\n",
        "    \n",
        "    new_all_words = list(set(all_words))\n",
        "    counts = dict(Counter(all_words))\n",
        "    # open file in write mode\n",
        "    with open(r'keywords.txt', 'w') as fp:\n",
        "        for item in counts:\n",
        "          try:\n",
        "            item = item +'\\t' +str(counts[item])\n",
        "          except:\n",
        "            item = item +'\\t' +str(1)\n",
        "          # write each item on a new line\n",
        "          fp.write(\"%s\\n\" % item)\n",
        "        print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1S5jyQ5mDN4",
        "outputId": "c906ae4c-a4e3-402e-9c5a-8e7ed6ebaef8"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/text_tech_classification/storage capacity_2001_3000.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cleaning data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 39529/39529 [00:01<00:00, 19855.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('storage', 5434), ('capacity', 2837), ('data', 2402), ('corp', 1957), ('application', 1427), ('method', 1403), ('system', 1371), ('memory', 1258), ('device', 1150), ('patent', 1023), ('power', 1006), ('description', 1000), ('use', 934), ('based', 921), ('energy', 896), ('advantage', 860), ('novelty', 770), ('unit', 735), ('information', 735), ('electric', 678)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = []\n",
        "file_ = os.path.join(files_location,'*.txt')\n",
        "files.extend(glob.glob(file_))\n",
        "\n",
        "for file_name in files:\n",
        "  folder_path = file_name[:-4]\n",
        "  if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "  os.chdir(folder_path)\n",
        "  print(folder_path)\n",
        "  process_file(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FyPrllt0LpQ",
        "outputId": "39500894-6bfd-40d0-9d4f-428237cf9625"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/text_tech_classification/storage capacity_2001_3000\n",
            "/content/drive/MyDrive/text_tech_classification/storage capacity_1_1000\n",
            "/content/drive/MyDrive/text_tech_classification/data transfer performance \n",
            "/content/drive/MyDrive/text_tech_classification/storage capacity_4001_5000\n",
            "/content/drive/MyDrive/text_tech_classification/storage capacity_1001_2000\n",
            "/content/drive/MyDrive/text_tech_classification/savedrecs\n",
            "/content/drive/MyDrive/text_tech_classification/storage capacity_3001_4000\n"
          ]
        }
      ]
    }
  ]
}